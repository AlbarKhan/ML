/////////////////////PRACTICAL 1///////////////////

Aim: Scrape an online Social Media Site for Data. Use python to scrape information from twitter.


Description:  
To implement the scraping of social media site like twitter to scrap the information, we'll follow these 
steps: 
1. Set up your Twitter Developer Account: Go to the Twitter Developer website and create 
developer account. 
2. Create a new application to get your API keys and access tokens. 
3. Install Tweepy, Ensure you have Tweepy installed. If not, you can install it using pip 
4. Write the Python script: Here's a simple script to scrape tweets from Twitter using Tweepy  

CODE:
import tweepy

# Twitter API credentials
consumer_key = 'hnFPOh0SIEZ3neOavSHPVCcB3'
consumer_secret = 'jtbVappjoePQyq7IK6ZRsQtrN5uN5VAp5VWbfAKzbyoUaIVoOX'
access_token = '1798945542866911232-knq5sXP9L2gB4IoB3ogwZKIekVTUkf'
access_token_secret = 'GrnLyzwO44ON7UCt8IR9Ze2BeC1aUWtfiBDoKTL80b9bh'

# Authenticate with Twitter
auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret, access_token, access_token_secret)
api = tweepy.API(auth)

# Define search parameters
search_words = "Eminem since:2023-01-01"
tweet_limit = 100

# Fetch tweets
try:
    tweets = tweepy.Cursor(
        api.search_tweets,
        q=search_words,
        lang="en",
        tweet_mode='extended'  # Use extended mode to get full text
    ).items(tweet_limit)

    # Save tweets to a file
    with open('tweets.txt', 'w', encoding='utf-8') as f:
        for tweet in tweets:
            output = f"Tweet by @{tweet.user.screen_name}: {tweet.full_text}\n\n"
            print(output)
            f.write(output)

except Exception as e:
    print(f"An error occurred: {e}")


////////////////////////PRACTICAL 2 //////////////////
Practical No. 2 
 
Aim: Page Rank for link analysis using python Create a small set of pages namely page1, page2, page3 
and page4 apply random walk on the same. 

CODE:

import numpy as np 
 
# Define links between pages 
links = { 
    'page1': ['page2', 'page3'], 
    'page2': ['page3'], 
    'page3': ['page1'], 
    'page4': ['page1', 'page3'] 
} 
 
# Initialization 
pages = list(links.keys()) 
n_pages = len(pages)
damping_factor = 0.85 
iterations = 100 
 
# Set initial PageRank values 
page_rank = {page: 1 / n_pages for page in pages} 
 
# Iterative calculation 
for _ in range(iterations): 
    new_page_rank = {page: (1 - damping_factor) / n_pages for page in pages} 
 
    for page, out_links in links.items(): 
        share = page_rank[page] / len(out_links) 
        for linked_page in out_links: 
            new_page_rank[linked_page] += damping_factor * share 
 
    page_rank = new_page_rank 
 
# Output final PageRank values 
for page, rank in page_rank.items(): 
    print(f'{page}: {rank:.4f}') 


////////////////////////////PRACTICAL 3////////////////////////
Practical No. 3 
 
Aim: Perform Spam Classifier. 

CODE:
import pandas as pd 
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.naive_bayes import MultinomialNB 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score 
 
# Load and prepare dataset 
df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']] 
df.columns = ['label', 'text'] 
df['label'] = df['label'].map({'ham': 0, 'spam': 1}) 
 
# Split data into training and test sets 
X_train, X_test, y_train, y_test = train_test_split( 
    df['text'], df['label'], test_size=0.2, random_state=42 
) 
# Vectorize text using TF-IDF 
vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7) 
X_train_tfidf = vectorizer.fit_transform(X_train) 
X_test_tfidf = vectorizer.transform(X_test) 
 
# Train a Naive Bayes classifier 
clf = MultinomialNB() 
clf.fit(X_train_tfidf, y_train) 
 
# Make predictions 
y_pred = clf.predict(X_test_tfidf) 
 
# Evaluate model performance 
accuracy = accuracy_score(y_test, y_pred) 
precision = precision_score(y_test, y_pred) 
recall = recall_score(y_test, y_pred) 
f1 = f1_score(y_test, y_pred) 
 
# Display metrics 
print(f'Accuracy : {accuracy:.4f}') 
print(f'Precision: {precision:.4f}') 
print(f'Recall   : {recall:.4f}') 
print(f'F1 Score : {f1:.4f}') 


////////////////////// PRACTICAL 4 /////////////////////////
Practical No. 4 
 
Aim: Demonstrate Text Mining and Webpage Pre-processing using meta information from the web 
pages (Local/Online)  

CODE:
import requests 
from bs4 import BeautifulSoup 
 
import nltk 
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
 
# Download required NLTK resources 
nltk.download('punkt') 
nltk.download('stopwords') 
nltk.download('wordnet') 
 
# Website to scrape 
url = 'https://www.nextgenpixel.co.in/' 
response = requests.get(url) 
 
if response.status_code == 200: 
    html_content = response.text 
 
    # Parse HTML content 
    soup = BeautifulSoup(html_content, 'html.parser') 
 
    # Extract title 
    title = soup.title.string if soup.title else 'No title' 
 
    # Extract meta description 
    description_tag = soup.find('meta', attrs={'name': 'description'}) 
    description = description_tag['content'] if description_tag else 'No description' 
 
    # Extract meta keywords 
    keywords_tag = soup.find('meta', attrs={'name': 'keywords'}) 
    keywords = keywords_tag['content'] if keywords_tag else 'No keywords' 
 
    # Display extracted metadata 
    print(f"Title      : {title}") 
    print(f"Description: {description}") 
    print(f"Keywords   : {keywords}") 
 
    # Combine metadata into one text block 
    text = f"{title} {description} {keywords}" 
 
    # Text preprocessing 
    tokens = word_tokenize(text) 
    tokens = [token.lower() for token in tokens] 
 
    stop_words = set(stopwords.words('english')) 
    tokens = [token for token in tokens if token not in stop_words] 
 
    lemmatizer = WordNetLemmatizer() 
    tokens = [lemmatizer.lemmatize(token) for token in tokens] 
 
    # Final cleaned text 
    processed_text = ' '.join(tokens) 
    print(f"\nProcessed Text:\n{processed_text}") 
 
else: 
    print(f"Failed to retrieve webpage. Status code: {response.status_code}") 

///////////////////////PRACTICAL 5///////////////////
practical No. 5 
 
Aim: Apriori Algorithm implementation in case study. 
CODE:
import pandas as pd 
from mlxtend.preprocessing import TransactionEncoder 
from mlxtend.frequent_patterns import apriori, association_rules 
 
# Sample transactional data 
data = { 
    'Transaction': [1, 2, 3, 4, 5], 
    'Items': [ 
        ['Milk', 'Bread', 'Butter'], 
        ['Bread', 'Butter'], 
        ['Milk', 'Bread'], 
        ['Milk', 'Bread', 'Butter', 'Eggs'], 
        ['Milk', 'Eggs'] 
    ] 
} 
 
# Convert to DataFrame 
df = pd.DataFrame(data) 
 
# One-hot encoding for Apriori 
te = TransactionEncoder()
import pandas as pd 
from mlxtend.preprocessing import TransactionEncoder 
from mlxtend.frequent_patterns import apriori, association_rules 
 
# Sample transactional data 
data = { 
    'Transaction': [1, 2, 3, 4, 5], 
    'Items': [ 
        ['Milk', 'Bread', 'Butter'], 
        ['Bread', 'Butter'], 
        ['Milk', 'Bread'], 
        ['Milk', 'Bread', 'Butter', 'Eggs'], 
        ['Milk', 'Eggs'] 
    ] 
} 
 
# Convert to DataFrame 
df = pd.DataFrame(data) 
 
# One-hot encoding for Apriori 
te = TransactionEncoder()
te_ary = te.fit_transform(df['Items']) 
df_encoded = pd.DataFrame(te_ary, columns=te.columns_) 
 
# Display encoded DataFrame 
print("One-hot encoded DataFrame:") 
print(df_encoded) 
 
# Find frequent itemsets 
frequent_itemsets = apriori(df_encoded, min_support=0.6, use_colnames=True) 
print("\nFrequent Itemsets:") 
print(frequent_itemsets) 
 
# Generate association rules 
rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7) 
print("\nAssociation Rules:") 
print(rules) 

/////////////////////////PRACTICAL 6//////////////////////

Practical No. 6 
 
Aim: Develop a basic crawler for the web search for user defined keywords. 
CODE:
import requests 
from bs4 import BeautifulSoup 
 
# Keywords to search for 
keywords = ["Python", "web scraping", "data science"] 
 
# Fetch a webpage's HTML 
def fetch_page(url): 
    try: 
        response = requests.get(url) 
        response.raise_for_status() 
        return response.text 
    except requests.RequestException as e: 
        print(f"Error fetching {url}: {e}") 
        return None 
 
# Parse HTML using BeautifulSoup 
def parse_page(html_content): 
    soup = BeautifulSoup(html_content, 'html.parser') 
    return soup
    
# Count occurrences of keywords in page text 
def search_keywords(soup, keywords): 
    text = soup.get_text() 
    found_keywords = { 
        keyword: text.lower().count(keyword.lower()) 
        for keyword in keywords 
    } 
    return found_keywords 
 
# Display keyword counts for a given URL 
def display_results(url, found_keywords): 
    print(f"Results for {url}:") 
    for keyword, count in found_keywords.items(): 
        print(f"  {keyword}: {count}") 
    print() 
 
# List of URLs to scrape 
urls = [ 
    "https://www.python.org", 
    "https://www.datasciencecentral.com", 
    "https://www.scrapinghub.com" 
] 
 
# Execute search for each URL 
for url in urls: 
    html_content = fetch_page(url) 
    if html_content: 
        soup = parse_page(html_content) 
        found_keywords = search_keywords(soup, keywords) 
        display_results(url, found_keywords) 

///////////////////////PRACTICAL 7 ///////////////////

Practical No. 7 
 
Aim: Develop a focused crawler for local search. 
CODE:
import requests 
from bs4 import BeautifulSoup 
from collections import deque 
 
keywords = ["Python", "web scraping", "data science"] 
seed_urls = ["https://www.python.org", "https://www.datasciencecentral.com", 
"https://www.scrapinghub.com"] 
 
# Function to fetch web pages 
def fetch_page(url): 
    try: 
        response = requests.get(url) 
        response.raise_for_status() 
        return response.text 
    except requests.RequestException as e:
       print(f"Error fetching {url}: {e}") 
       return None 
 
# Function to parse HTML content 
def parse_page(html_content): 
    soup = BeautifulSoup(html_content, 'html.parser') 
    return soup 
 
# Function to search for keywords in the content 
def search_keywords(soup, keywords): 
    text = soup.get_text() 
    found_keywords = {keyword: text.lower().count(keyword.lower()) for keyword in keywords} 
    return found_keywords 
 
# Function to extract links from the page 
def extract_links(soup, base_url): 
    links = set() 
    for link in soup.find_all('a', href=True): 
        url = link['href'] 
        if url.startswith('http'): 
            links.add(url) 
        elif url.startswith('/'): 
            links.add(requests.compat.urljoin(base_url, url)) 
    return links 
 
# Function to display the results 
def display_results(url, found_keywords): 
    print(f"Results for {url}:") 
    for keyword, count in found_keywords.items(): 
        print(f"  {keyword}: {count}") 
    print() 
 
# Focused crawler implementation 
def focused_crawler(seed_urls, keywords, max_pages=20): 
    crawled_urls = set() 
    urls_to_crawl = deque(seed_urls) 
 
    while urls_to_crawl and len(crawled_urls) < max_pages: 
        url = urls_to_crawl.popleft() 
        if url in crawled_urls: 
            continue 
 
        html_content = fetch_page(url) 
        if not html_content: 
            continue 
 
        soup = parse_page(html_content)
        found_keywords = search_keywords(soup, keywords) 
        display_results(url, found_keywords) 
 
        if any(count > 0 for count in found_keywords.values()): 
            new_links = extract_links(soup, url) 
            urls_to_crawl.extend(new_links - crawled_urls) 
 
        crawled_urls.add(url) 
 
# Start crawling 
focused_crawler(seed_urls, keywords) 